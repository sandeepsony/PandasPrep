{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1: House Price Prediction\n",
    "## 1. Problem Definition\n",
    "### Predict house prices based on feature like size, location, and amenities. \n",
    "### We will use linear regression model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Boston data which is available in scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Load the california housing data\n",
    "housing = fetch_california_housing()\n",
    "data = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "data['PRICE'] = housing.target\n",
    "#display(data)\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nFeature names: {housing.feature_names}\")\n",
    "print(f\"\\nFirst 5 rows of our dataset: {data.head()}\")\n",
    "print(f\"\\nStatistical summary: {data.describe()}\")\n",
    "\n",
    "# Missing values -  NO MISSING VALUE\n",
    "print(f\"Missing values in each column: {data.isnull().sum()}\")\n",
    "\n",
    "# Visualize the distribution of house prices -HISTOGRAM\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data['PRICE'], kde=True, )\n",
    "plt.title('Distribution of House Prices')\n",
    "plt.xlabel('Prices (x $100k)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "#Correlation matrix to see relationship between features\n",
    "correlation_matrix = data.corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "#Split the data into features (X) and target (y)\n",
    "X=data.drop('PRICE', axis=1)\n",
    "y=data['PRICE']\n",
    "# Split into tarining and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "#Train the model on training data\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred = linear_model.predict(X_test)\n",
    "#Evaluate the model using test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Linear Regression Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Let's see the coefficients to understand feature importance\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': linear_model.coef_\n",
    "})\n",
    "coefficients = coefficients.sort_values(by='Coefficient', ascending=False)\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use LAZYPREDICT to see the top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LazyPredict to try multiple models quickly\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "# Initialize LazyRegressor\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "# Fit and evaluate multiple models\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Display performance comparison of various models\n",
    "print(\"\\nLazyPredict Model Comparison:\")\n",
    "print(models)\n",
    "\n",
    "# Let's visualize the top performing models\n",
    "plt.figure(figsize=(12, 6))\n",
    "models_r2 = models.sort_values(by='R-Squared', ascending=False).head(10)\n",
    "sns.barplot(x=models_r2.index, y=models_r2['R-Squared'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 10 Models by R-Squared')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We identified XGBoost is the top performing model, so we will train and hyperparameter tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import XGB\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "#Initialize the XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(random=42)\n",
    "# check basic performance with cross-validation\n",
    "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"XGBoost cross-validation R2 score: {cv_scores}\")\n",
    "print(f\"Mean CV R2 score: {cv_scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(xgb_mse)\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "xgb.plot_importance(xgb_model)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200],\n",
    "    'max_depth': [3,5,7],\n",
    "    'learning_rate': [0.1,0.05]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator= xgb.XGBRegressor(random=42),\n",
    "    param_grid=param_grid,\n",
    "    cv = 3,\n",
    "    scoring='r2',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fiting the grid search and getting result\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best R2 Score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model with best parameters\n",
    "best_xgb = grid_search.best_estimator_\n",
    "best_xgb.fit(X_train, y_train)\n",
    "# Final Evaluation\n",
    "final_pred = best_xgb.predict(X_test)\n",
    "final_r2 = r2_score(y_test, final_pred)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, final_pred))\n",
    "#Print final results\n",
    "print(f\"Root Mean Squared Error: {final_rmse:.4f}\")\n",
    "print(f\"R² Score: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISULIZE\n",
    "# Create a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': final_pred\n",
    "})\n",
    "# Calculate prediction errors\n",
    "results_df['Error'] = results_df['Actual'] - results_df['Predicted']\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Prices ($100,000s)')\n",
    "plt.ylabel('Predicted Prices ($100,000s)')\n",
    "plt.title('Actual vs Predicted House Prices')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(results_df['Error'], kde=True)\n",
    "plt.xlabel('Prediction Error ($100,000s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize errors vs predicted values (to check for patterns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(results_df['Predicted'], results_df['Error'], alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Prices ($100,000s)')\n",
    "plt.ylabel('Prediction Error ($100,000s)')\n",
    "plt.title('Prediction Errors vs Predicted Values')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import lime \n",
    "from lime import lime_tabular\n",
    "\n",
    "# AS we already have our trained model XGBoost model (best_xgb) and our test data X_test\n",
    "\n",
    "# 1. SHAP (SHapley Additive exPlanations)\n",
    "#create a SHAP explainer for our model\n",
    "explainer = shap.TreeExplainer(best_xgb)\n",
    "shap_values = explainer.shap_values(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SHAP values for a single prediction\n",
    "plt.figure(figsize=(12,6))\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:],\n",
    "                feature_names=X_test.columns, matplotlib=True)\n",
    "plt.title(\"SHAP Force plot for a single prediction\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary plot showing impact of all features\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.title(\"SHAP Summary Plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from nonconformist.cp import IcpRegressor\n",
    "from nonconformist.base import RegressorAdapter\n",
    "from nonconformist.nc import RegressorNc, AbsErrorErrFunc\n",
    "\n",
    "# Assuming we have our data from Project 1\n",
    "# Convert pandas DataFrames to NumPy arrays\n",
    "X_np = X.values\n",
    "y_np = y.values\n",
    "\n",
    "# Split data into training, calibration, and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "X_train, X_calib, y_train, y_calib = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Calibration set: {X_calib.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create an XGBoost model (our best model from Project 1)\n",
    "model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make point predictions on the test set\n",
    "point_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate baseline metrics\n",
    "baseline_rmse = np.sqrt(np.mean((y_test - point_predictions) ** 2))\n",
    "print(f\"Baseline RMSE without conformal prediction: {baseline_rmse:.4f}\")\n",
    "\n",
    "# Create a nonconformist adapter that wraps our model\n",
    "adapter = RegressorAdapter(model)\n",
    "\n",
    "# Create a nonconformity function - we use absolute error\n",
    "nc = RegressorNc(adapter, AbsErrorErrFunc())\n",
    "\n",
    "# Create an inductive conformal predictor (ICP)\n",
    "icp = IcpRegressor(nc)\n",
    "\n",
    "# Calibrate the conformal predictor using the calibration set\n",
    "icp.fit(X_calib, y_calib)\n",
    "\n",
    "# Generate prediction intervals at 90% confidence\n",
    "significance = 0.1  # This means 90% confidence (1 - 0.1)\n",
    "prediction_intervals = icp.predict(X_test, significance=significance)\n",
    "\n",
    "# Calculate the width of each interval\n",
    "interval_widths = prediction_intervals[:, 1] - prediction_intervals[:, 0]\n",
    "average_width = np.mean(interval_widths)\n",
    "\n",
    "# Calculate coverage (percentage of true values inside intervals)\n",
    "coverage = np.mean((y_test >= prediction_intervals[:, 0]) & \n",
    "                    (y_test <= prediction_intervals[:, 1]))\n",
    "\n",
    "print(f\"90% Prediction Interval - Average width: {average_width:.4f}\")\n",
    "print(f\"90% Prediction Interval - Empirical coverage: {coverage:.4f}\")\n",
    "\n",
    "# Display sample predictions with intervals\n",
    "print(\"\\nSample predictions with intervals:\")\n",
    "for i in range(5):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"True value: {y_test[i]:.4f}\")\n",
    "    print(f\"Predicted value: {point_predictions[i]:.4f}\")\n",
    "    print(f\"90% prediction interval: [{prediction_intervals[i, 0]:.4f}, {prediction_intervals[i, 1]:.4f}]\")\n",
    "    print(f\"Interval width: {interval_widths[i]:.4f}\")\n",
    "    print(f\"True value within interval: {(y_test[i] >= prediction_intervals[i, 0]) and (y_test[i] <= prediction_intervals[i, 1])}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
